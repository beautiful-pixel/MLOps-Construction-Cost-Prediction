# MLOps Platform â€“ Construction Cost Prediction

This project implements an end-to-end MLOps platform for the Solafune Construction Cost Prediction challenge.

The current production baseline is tabular-only.  
Image ingestion, validation and versioning are implemented to prepare for future multimodal modeling.

The objective is not only model performance, but the design of a reproducible, modular, production-oriented ML system covering the full lifecycle:
data ingestion â†’ validation â†’ training â†’ promotion â†’ serving â†’ monitoring.

---

# Project Objectives

## Machine Learning

- Predict regional construction costs (tabular baseline)
- Prepare multimodal learning (satellite imagery pipeline ready)
- Track experiments reproducibly

## MLOps

- Modular microservice architecture
- Versioned configurations (feature / model / split)
- Data versioning with DVC
- Experiment tracking with MLflow
- Orchestrated pipelines with Airflow
- Secure model serving via FastAPI
- Reverse proxy entrypoint (Nginx)
- Monitoring with Prometheus & Grafana
- CI & automated testing

---

# Architecture Overview

The system is structured around clearly separated responsibilities:

âš« **Entry Layer** â†’ Nginx (HTTPS reverse proxy)  
ðŸ”µ **Serving Layer** â†’ Streamlit (app), Gateway API, Inference API  
ðŸŸ¢ **Training Layer** â†’ Airflow, MLflow, PostgreSQL  
ðŸŸ  **Monitoring Layer** â†’ Prometheus, Grafana

## High-Level Service Diagram

```mermaid
%%{init: {'theme':'base'}}%%
flowchart TB

    Nginx["Nginx (HTTPS Entry Point)"]
    Streamlit["Streamlit UI"]
    Gateway["Gateway API"]
    Inference["Inference API"]
    Airflow["Airflow"]
    MLflow["MLflow Registry"]
    Postgres["PostgreSQL"]
    Prometheus["Prometheus"]
    Grafana["Grafana"]

    Nginx -->|/| Streamlit
    Nginx -->|/api| Gateway
    Nginx -->|/grafana| Grafana
    Streamlit --> Gateway
    Inference -->|Load prod model| MLflow
    Gateway -->|List runs| MLflow
    Gateway -->|Trigger training| Airflow
    Gateway -->|Predict| Inference
    Airflow -->|Log & register model| MLflow
    MLflow --> Postgres
    Airflow -->|Reload after promote| Inference
    Prometheus -->|Scrape| Gateway
    Prometheus -->|Scrape| Inference
    Grafana --> Prometheus

    %% Entry Layer
    style Nginx fill:#eeeeee,stroke:#616161,stroke-width:2px

    %% Serving Layer
    style Streamlit fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    style Gateway fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px
    style Inference fill:#e3f2fd,stroke:#1e88e5,stroke-width:2px

    %% Training Layer
    style Airflow fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style MLflow fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style Postgres fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px

    %% Monitoring Layer
    style Prometheus fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
    style Grafana fill:#fff3e0,stroke:#ef6c00,stroke-width:2px
```

---

# Live Deployment

# Live Deployment

The platform is deployed on an Oracle Cloud server and orchestrated with Kubernetes.  
It is publicly accessible at:

https://engineerai.space

The following routes are exposed:

- `/` â†’ Streamlit application  
- `/api` â†’ Gateway API  
- `/grafana` â†’ Monitoring dashboards  

Authentication is required to access protected services.

The Streamlit application acts strictly as a frontend client of the Gateway API.  
It does not communicate directly with Airflow, MLflow, or the Inference service.  
All operational and inference requests pass through the Gateway layer.

---

# API Gateway

The API Gateway is the central orchestration layer of the platform.

All external interactions pass through the gateway.  
No internal service (Airflow, MLflow, Inference API) is exposed directly.

The gateway acts as:

- A control plane for ML operations  
- A secure abstraction layer over Airflow and MLflow  
- A single entry point for both UI and programmatic access  

Full interactive API documentation is available at https://engineerai.space/api/docs

---

# Prediction Flow

1. User interacts with Streamlit App
2. Request passes through Nginx (HTTPS)
3. Gateway authenticates, validates, and proxies the request
4. Inference API performs prediction
5. Model loaded from MLflow alias `prod`
6. Prediction returned to user

---

# Model Lifecycle

Model training, evaluation, promotion, and inference reload are fully automated through the Airflow orchestration layer.

The detailed lifecycle â€” including dataset splitting, MLflow tracking, DVC lineage logging, promotion logic, and registry alias management â€” is documented in:

- `docs/train_pipeline.md`
- `docs/data_pipeline.md`

This separation keeps the README focused on system architecture while maintaining detailed technical traceability in dedicated documentation.

---

# Repository Structure

```
mlops-project/

â”œâ”€â”€ api/                           # FastAPI microservices
â”‚   â”œâ”€â”€ gateway_api/               # API gateway (auth, orchestration)
â”‚   â””â”€â”€ inference_api/             # Model serving microservice
â”‚
â”œâ”€â”€ src/                           # Core ML business library (Python package)
â”‚   â”œâ”€â”€ data/                      # Data ingestion & validation logic
â”‚   â”œâ”€â”€ features/                  # Feature schema & preprocessing pipelines
â”‚   â”œâ”€â”€ models/                    # Model schema & MLflow loader
â”‚   â”œâ”€â”€ inference/                 # Dynamic request schema builder
â”‚   â”œâ”€â”€ registry/                  # MLflow registry utilities
â”‚   â”œâ”€â”€ splitting/                 # Train/test split orchestration
â”‚   â”œâ”€â”€ pipelines/                 # Data & training pipelines (modular)
â”‚   â”œâ”€â”€ training/                  # Metrics & training utilities
â”‚   â””â”€â”€ utils/                     # Config resolution, DVC, logging helpers
â”‚
â”œâ”€â”€ dags/                          # Airflow DAG definitions
â”‚   â”œâ”€â”€ data_pipeline_dag.py
â”‚   â”œâ”€â”€ train_pipeline_dag.py
â”‚   â””â”€â”€ retrain_policy_dag.py
â”‚
â”œâ”€â”€ configs/                       # Versioned YAML configurations
â”‚   â”œâ”€â”€ active_config.yaml
â”‚   â”œâ”€â”€ data_contracts/
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ splits/
â”‚
â”œâ”€â”€ app/                           # Streamlit dashboard (multi-page UI)
â”‚
â”œâ”€â”€ deployments/                   # Docker & Kubernetes manifests
â”‚
â”œâ”€â”€ data/                          # DVC versioned datasets
â”‚   â”œâ”€â”€ incoming/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ splits/
â”‚   â””â”€â”€ reference/
â”‚
â”œâ”€â”€ tests/                         # Unit & integration tests
â”‚
â””â”€â”€ mlflow_server/                 # MLflow backend store & artifacts
```

---

# Evaluation Metrics

Primary metric:
- RMSLE (official Solafune metric)

Additional metrics:
- MAE
- RÂ²

All metrics are logged in MLflow.

---

# Monitoring

- Prometheus scrapes API metrics
- Grafana provides dashboards
- Accessible via `/grafana` behind Nginx

---

# Deployment

Development:

```bash
docker compose -f deployments/compose.yaml -f deployments/compose.dev.yaml up
```

Production:

```bash
docker compose -f deployments/compose.yaml up -d
```

Environment variables managed via `.env`.

---

# Key MLOps Capabilities

- Versioned configuration system
- Model registry with aliasing
- Automated promotion logic
- Immediate model reload
- Data versioning with DVC
- Containerized microservices
- Observability stack
- Clear separation of concerns

---

This project demonstrates a clean, maintainable, production-oriented MLOps architecture, 
designed for internal ML platform usage rather than leaderboard optimization.